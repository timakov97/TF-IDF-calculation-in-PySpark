{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: /user/st054328/\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/user/st054328/proxy/4040/jobs/\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f34f451da00>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import boto3\n",
    "import sklearn\n",
    "import socket\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "#from pyspark.sql import types as T\n",
    "\n",
    "print('user:', os.environ['JUPYTERHUB_SERVICE_PREFIX'])\n",
    "\n",
    "def uiWebUrl(self):\n",
    "    from urllib.parse import urlparse\n",
    "    web_url = self._jsc.sc().uiWebUrl().get()\n",
    "    port = urlparse(web_url).port\n",
    "    return \"{}proxy/{}/jobs/\".format(os.environ['JUPYTERHUB_SERVICE_PREFIX'], port)\n",
    "\n",
    "# small fix to enable UI views\n",
    "SparkContext.uiWebUrl = property(uiWebUrl)\n",
    "\n",
    "# spark configurtion in local regime \n",
    "conf = SparkConf().set('spark.master', 'local[*]').set('spark.driver.memory', '8g')\n",
    "\n",
    "#some needed objects\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1\n",
    "\n",
    "Transform text file \"The Project Gutenberg eBook of Frankenstein, by Mary Wollstonecraft (Godwin) Shelle\" into TF-IDF. Take sentence as \"document\".\n",
    "\n",
    "### Part 1: \n",
    "- read text file as dataframe \n",
    "- filter out non-letters and empty strings \n",
    "- transform into dataframe doc_id -> tf_idf vector \n",
    "\n",
    "\n",
    "### Part 2:\n",
    "- read text file as RDD\n",
    "- filter out non-letters and empty strings \n",
    "- transform into rdd in format doc_id -> tf_idf vector\n",
    "\n",
    "\n",
    "### Org part: \n",
    "I'm waiting your HW's as self-sufficient jupyter notebooks in github repository. \n",
    "\n",
    "Please, fill this table in specified comment with:\n",
    "\n",
    "your name / github link / telegram (optionally, if u want to discuss some) / \n",
    "\n",
    "Fill the comment please and i will add your data in a few days\n",
    "\n",
    "https://docs.google.com/spreadsheets/d/1p3yLsXqX2dp_TrPwNcikcS5FP_PTM0_gnSOzGn5Gn1E/edit#gid=0\n",
    "\n",
    "Feel free to text me if u have some questions \n",
    "\n",
    "### Deadline: 01.05.2021 included\n",
    "\n",
    "Dear students, dead in \"deadline\" means *dead*. This deadline is not for you - it's for me. Deadlines informs me from which point i should start to score your HWs.  You can commit anything after deadline but it's not guaranteed that I'll take it into account. It's possible to move deadline only for the whole group not \"just for me plz cause I was ill / detentioned / skipped this message\". \n",
    "\n",
    "### NB(!) \n",
    "\n",
    "It's not allowed to use TF-IDF code from Spark internal libraries. \n",
    "It's not allowed to cast DF/RDD into pandas and use scikit-learn. Please, keep it spark. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "\n",
    "Reading text file as dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+\n",
      "|                text| id|\n",
      "+--------------------+---+\n",
      "|The Project Guten...|  0|\n",
      "|                    |  1|\n",
      "|This eBook is for...|  2|\n",
      "|most other parts ...|  3|\n",
      "|whatsoever. You m...|  4|\n",
      "|of the Project Gu...|  5|\n",
      "|www.gutenberg.org...|  6|\n",
      "|will have to chec...|  7|\n",
      "|   using this eBook.|  8|\n",
      "|                    |  9|\n",
      "| Title: Frankenstein| 10|\n",
      "|       or, The Mo...| 11|\n",
      "|                    | 12|\n",
      "|Author: Mary Woll...| 13|\n",
      "|                    | 14|\n",
      "|Release Date: 31,...| 15|\n",
      "|[Most recently up...| 16|\n",
      "|                    | 17|\n",
      "|   Language: English| 18|\n",
      "|                    | 19|\n",
      "+--------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_prefix = \"malyutin_demo_hw1\"\n",
    "\n",
    "filepath = \"file:///home/jovyan/shared/lectures_folder/84-0.txt\"\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "dataframe = sc.textFile(f\"{filepath}\")\\\n",
    "    .map(lambda x: (x,))\\\n",
    "    .toDF()\\\n",
    "    .select(F.col(\"_1\").alias(\"text\"))\\\n",
    "    .withColumn(\"id\", monotonically_increasing_id())\n",
    "\n",
    "dataframe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType, MapType\n",
    "from pyspark.mllib.linalg import Vectors, DenseVector\n",
    "\n",
    "import string\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def process_string(data):\n",
    "    \"\"\"\n",
    "    basic preprocessing function:\n",
    "    - removes punctuation\n",
    "    - lower\n",
    "    - split by space\n",
    "    \"\"\"\n",
    "    punct_removed = re.sub(r'[^a-zA-Z ]','',data)\n",
    "    words = punct_removed.lower().split(\" \")\n",
    "    \n",
    "    \n",
    "    return list(filter(lambda x: len(x) > 0, words))\n",
    "\n",
    "# spark udf -- user defined function (~ mapper)\n",
    "\n",
    "process_string_udf = udf(lambda z: process_string(z), ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+\n",
      "|                text| id|\n",
      "+--------------------+---+\n",
      "|[the, project, gu...|  0|\n",
      "|[this, ebook, is,...|  2|\n",
      "|[most, other, par...|  3|\n",
      "|[whatsoever, you,...|  4|\n",
      "|[of, the, project...|  5|\n",
      "|[wwwgutenbergorg,...|  6|\n",
      "|[will, have, to, ...|  7|\n",
      "|[using, this, ebook]|  8|\n",
      "|[title, frankenst...| 10|\n",
      "|[or, the, modern,...| 11|\n",
      "|[author, mary, wo...| 13|\n",
      "|[release, date, e...| 15|\n",
      "|[most, recently, ...| 16|\n",
      "| [language, english]| 18|\n",
      "|[character, set, ...| 20|\n",
      "|[produced, by, ju...| 22|\n",
      "|[further, correct...| 23|\n",
      "|[start, of, the, ...| 25|\n",
      "|      [frankenstein]| 30|\n",
      "|[or, the, modern,...| 32|\n",
      "+--------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "process words\n",
    "\n",
    "filter out empty and small sentences\n",
    "\"\"\"\n",
    "\n",
    "by_words = dataframe\\\n",
    "    .select(process_string_udf(F.col(\"text\")).alias(\"text\"), \"id\")\\\n",
    "    .where(F.size(F.col(\"text\")) >= 1)#\\\n",
    "    #.withColumn(\"id\", monotonically_increasing_id())\n",
    "\n",
    "\n",
    "by_words.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explode the words in each text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+--------------+---+\n",
      "|            document|doc_id|         token| id|\n",
      "+--------------------+------+--------------+---+\n",
      "|[the, project, gu...|     0|           the|  0|\n",
      "|[the, project, gu...|     0|       project|  1|\n",
      "|[the, project, gu...|     0|     gutenberg|  2|\n",
      "|[the, project, gu...|     0|         ebook|  3|\n",
      "|[the, project, gu...|     0|            of|  4|\n",
      "|[the, project, gu...|     0|  frankenstein|  5|\n",
      "|[the, project, gu...|     0|            by|  6|\n",
      "|[the, project, gu...|     0|          mary|  7|\n",
      "|[the, project, gu...|     0|wollstonecraft|  8|\n",
      "|[the, project, gu...|     0|        godwin|  9|\n",
      "|[the, project, gu...|     0|       shelley| 10|\n",
      "|[this, ebook, is,...|     2|          this| 11|\n",
      "|[this, ebook, is,...|     2|         ebook| 12|\n",
      "|[this, ebook, is,...|     2|            is| 13|\n",
      "|[this, ebook, is,...|     2|           for| 14|\n",
      "|[this, ebook, is,...|     2|           the| 15|\n",
      "|[this, ebook, is,...|     2|           use| 16|\n",
      "|[this, ebook, is,...|     2|            of| 17|\n",
      "|[this, ebook, is,...|     2|        anyone| 18|\n",
      "|[this, ebook, is,...|     2|      anywhere| 19|\n",
      "+--------------------+------+--------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unfolded = by_words\\\n",
    ".select(by_words.text.alias('document'), by_words.id.alias('doc_id'), F.explode(by_words.text).alias('token'))\\\n",
    ".withColumn(\"id\", monotonically_increasing_id())\n",
    "\n",
    "unfolded.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's count the number of each certain word `t` in the text and the total number of words `n` in each text. Then let's calculate `TF` as $t/n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+---+---+-------------------+\n",
      "|doc_id|         token|  t|  n|                 TF|\n",
      "+------+--------------+---+---+-------------------+\n",
      "|     0|     gutenberg|  1| 11|0.09090909090909091|\n",
      "|     0|  frankenstein|  1| 11|0.09090909090909091|\n",
      "|     0|wollstonecraft|  1| 11|0.09090909090909091|\n",
      "|     0|         ebook|  1| 11|0.09090909090909091|\n",
      "|     0|       shelley|  1| 11|0.09090909090909091|\n",
      "|     0|            by|  1| 11|0.09090909090909091|\n",
      "|     0|        godwin|  1| 11|0.09090909090909091|\n",
      "|     0|            of|  1| 11|0.09090909090909091|\n",
      "|     0|       project|  1| 11|0.09090909090909091|\n",
      "|     0|          mary|  1| 11|0.09090909090909091|\n",
      "|     0|           the|  1| 11|0.09090909090909091|\n",
      "|     2|          this|  1| 13|0.07692307692307693|\n",
      "|     2|        united|  1| 13|0.07692307692307693|\n",
      "|     2|            of|  1| 13|0.07692307692307693|\n",
      "|     2|           and|  1| 13|0.07692307692307693|\n",
      "|     2|      anywhere|  1| 13|0.07692307692307693|\n",
      "|     2|            in|  1| 13|0.07692307692307693|\n",
      "|     2|            is|  1| 13|0.07692307692307693|\n",
      "|     2|           the|  2| 13|0.15384615384615385|\n",
      "|     2|        states|  1| 13|0.07692307692307693|\n",
      "+------+--------------+---+---+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf0 = unfolded.groupBy(\"doc_id\", \"token\").agg(F.count(\"document\").alias(\"t\"))\n",
    "\n",
    "tf = tf0\\\n",
    ".join(tf0.groupBy(\"doc_id\").agg(F.count(\"doc_id\").alias(\"n\")), on=\"doc_id\", how = \"left\")\\\n",
    ".orderBy(\"doc_id\")\\\n",
    ".withColumn(\"TF\", F.col(\"t\")/F.col(\"n\"))\n",
    "\n",
    "tf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's calculate the numbers of documents `df` containing each word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+\n",
      "|token|  df|\n",
      "+-----+----+\n",
      "|  the|3282|\n",
      "|  and|2702|\n",
      "|   of|2435|\n",
      "|    i|2356|\n",
      "|   to|1901|\n",
      "|   my|1534|\n",
      "|    a|1311|\n",
      "|   in|1127|\n",
      "| that| 971|\n",
      "|  was| 949|\n",
      "|   me| 795|\n",
      "| with| 694|\n",
      "|  but| 681|\n",
      "|  had| 649|\n",
      "|which| 554|\n",
      "|  you| 550|\n",
      "|   he| 547|\n",
      "|   it| 533|\n",
      "|  not| 519|\n",
      "|  for| 505|\n",
      "+-----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = unfolded.groupBy(\"token\").agg(F.countDistinct(\"doc_id\").alias(\"df\")).orderBy(\"df\", ascending=False)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's devide the total number of documents `D` by the number of documents containing each word `df`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------------------+\n",
      "|token|  df|              D/df|\n",
      "+-----+----+------------------+\n",
      "|  the|3282|2.0527117611212677|\n",
      "|  and|2702| 2.493338267949667|\n",
      "|   of|2435| 2.766735112936345|\n",
      "|    i|2356| 2.859507640067912|\n",
      "|   to|1901| 3.543924250394529|\n",
      "|   my|1534| 4.391786179921773|\n",
      "|    a|1311| 5.138825324180015|\n",
      "|   in|1127| 5.977817213842059|\n",
      "| that| 971| 6.938208032955716|\n",
      "|  was| 949| 7.099051633298209|\n",
      "|   me| 795| 8.474213836477988|\n",
      "| with| 694| 9.707492795389049|\n",
      "|  but| 681|   9.8928046989721|\n",
      "|  had| 649|10.380585516178737|\n",
      "|which| 554|12.160649819494585|\n",
      "|  you| 550| 12.24909090909091|\n",
      "|   he| 547|12.316270566727605|\n",
      "|   it| 533|12.639774859287055|\n",
      "|  not| 519|12.980732177263969|\n",
      "|  for| 505|13.340594059405941|\n",
      "+-----+----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d_df = df.withColumn('D/df', unfolded.select(F.countDistinct(\"doc_id\").alias('len')).collect()[0]['len']/df.df)\n",
    "d_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's take 10-based logarithm of `D/df` to get `IDF`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------------------+-------------------+\n",
      "|token|  df|              D/df|                IDF|\n",
      "+-----+----+------------------+-------------------+\n",
      "|  the|3282|2.0527117611212677| 0.3123279706356039|\n",
      "|  and|2702| 2.493338267949667| 0.3967812026666665|\n",
      "|   of|2435| 2.766735112936345| 0.4419675818020251|\n",
      "|    i|2356| 2.859507640067912|0.45629126123761427|\n",
      "|   to|1901| 3.543924250394529| 0.5494844304872352|\n",
      "|   my|1534| 4.391786179921773| 0.6426411877397161|\n",
      "|    a|1311| 5.138825324180015|  0.710863855662594|\n",
      "|   in|1127| 5.977817213842059| 0.7765426313065718|\n",
      "| that| 971| 6.938208032955716| 0.8412473174446734|\n",
      "|  was| 949| 7.099051633298209| 0.8512003349253856|\n",
      "|   me| 795| 8.474213836477988|  0.928099418696208|\n",
      "| with| 694| 9.707492795389049| 0.9871070768978234|\n",
      "|  but| 681|   9.8928046989721| 0.9953194354398932|\n",
      "|  had| 649|10.380585516178737|  1.016221850552309|\n",
      "|which| 554|12.160649819494585| 1.0849567826242486|\n",
      "|  you| 550| 12.24909090909091| 1.0881038578584346|\n",
      "|   he| 547|12.316270566727605| 1.0904792210192473|\n",
      "|   it| 533|12.639774859287055|  1.101739338326106|\n",
      "|  not| 519|12.980732177263969| 1.1132991895042204|\n",
      "|  for| 505|13.340594059405941| 1.1251751692340168|\n",
      "+-----+----+------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idf = d_df.withColumn('IDF', F.log(10.0, F.col('D/df')))\n",
    "idf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate `TF_IDF`, let's multiply `TF` and `IDF`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+---+--------------------+\n",
      "|         token|doc_id|  n|              TF_IDF|\n",
      "+--------------+------+---+--------------------+\n",
      "|            of|     0| 11|0.040178871072911376|\n",
      "|wollstonecraft|     0| 11|  0.3046677538757287|\n",
      "|          mary|     0| 11|  0.3046677538757287|\n",
      "|        godwin|     0| 11|  0.3046677538757287|\n",
      "|           the|     0| 11|0.028393451875963992|\n",
      "|       shelley|     0| 11|  0.3046677538757287|\n",
      "|       project|     0| 11| 0.17127126138204635|\n",
      "|         ebook|     0| 11| 0.24677483591325833|\n",
      "|     gutenberg|     0| 11| 0.21246407759258232|\n",
      "|            by|     0| 11| 0.10429320999791737|\n",
      "|  frankenstein|     0| 11| 0.21509714085942927|\n",
      "|        states|     2| 13| 0.19441819628374596|\n",
      "|           use|     2| 13|   0.192788250201443|\n",
      "|            is|     2| 13| 0.10339743907737634|\n",
      "|            in|     2| 13|0.059734048562043986|\n",
      "|            of|     2| 13|0.033997506292463474|\n",
      "|        anyone|     2| 13| 0.22109415676256564|\n",
      "|           the|     2| 13| 0.04805045702086214|\n",
      "|          this|     2| 13| 0.09100340600027589|\n",
      "|      anywhere|     2| 13|  0.2713412732068229|\n",
      "+--------------+------+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf_idf = tf\\\n",
    ".join(idf, on='token', how='left')\\\n",
    ".orderBy(\"doc_id\")\\\n",
    ".withColumn('TF_IDF', F.col(\"tf\") * F.col(\"idf\"))\\\n",
    ".select(\"token\", \"doc_id\", \"n\", \"TF_IDF\")\n",
    "\n",
    "tf_idf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's recover the order of words in text..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+---+---+--------------------+\n",
      "|doc_id|         token| id|  n|              TF_IDF|\n",
      "+------+--------------+---+---+--------------------+\n",
      "|     0|           the|  0| 11|0.028393451875963992|\n",
      "|     0|       project|  1| 11| 0.17127126138204635|\n",
      "|     0|     gutenberg|  2| 11| 0.21246407759258232|\n",
      "|     0|         ebook|  3| 11| 0.24677483591325833|\n",
      "|     0|            of|  4| 11|0.040178871072911376|\n",
      "|     0|  frankenstein|  5| 11| 0.21509714085942927|\n",
      "|     0|            by|  6| 11| 0.10429320999791737|\n",
      "|     0|          mary|  7| 11|  0.3046677538757287|\n",
      "|     0|wollstonecraft|  8| 11|  0.3046677538757287|\n",
      "|     0|        godwin|  9| 11|  0.3046677538757287|\n",
      "|     0|       shelley| 10| 11|  0.3046677538757287|\n",
      "|     2|          this| 11| 13| 0.09100340600027589|\n",
      "|     2|         ebook| 12| 13|  0.2088094765419878|\n",
      "|     2|            is| 13| 13| 0.10339743907737634|\n",
      "|     2|           for| 14| 13| 0.08655193609492438|\n",
      "|     2|           the| 15| 13| 0.04805045702086214|\n",
      "|     2|           use| 16| 13|   0.192788250201443|\n",
      "|     2|            of| 17| 13|0.033997506292463474|\n",
      "|     2|        anyone| 18| 13| 0.22109415676256564|\n",
      "|     2|      anywhere| 19| 13|  0.2713412732068229|\n",
      "+------+--------------+---+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf_idf2 = unfolded.select(\"id\", \"doc_id\", \"token\").join(tf_idf, on=[\"doc_id\", \"token\"], how=\"left\").orderBy(\"id\")\n",
    "tf_idf2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried to transform this dataframe into the format doc_id -> tf_idf vector, but didn't succeed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tqdm import tqdm\n",
    "\n",
    "#tfidfs = []\n",
    "#for i in tqdm(indicies):\n",
    "    #df = tf_idf2.where(F.col('doc_id')==i).select(\"TF_IDF\")\n",
    "    #tfidfs.append([(row.TF_IDF) for row in df.collect()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So I tried a little bit different approach.\n",
    "\n",
    "Let's save the IDFs, we've already calculated into the dictionary `dict_dict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_list = [{row.token : row.IDF} for row in idf.collect()]\n",
    "\n",
    "dict_dict = {}\n",
    "for element in dict_list:\n",
    "    dict_dict.update(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define a function returning a sparse vector with the values of TF-IDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "numFeatures = len(dict_dict) #the number of features equals the total number of words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(document):\n",
    "    freq = {}\n",
    "    tfidfs = {}\n",
    "    for term in document:\n",
    "        i = hash(term) % numFeatures #creating \"index\" for the word\n",
    "        freq[i] = freq.get(i, 0) + 1.0 #calculate the number of occurances of each word in a text\n",
    "        tfidfs[i] = freq[i]/len(document) * dict_dict[term] #calculate TF and multiply it by previously calculated IDF\n",
    "    return [document, Vectors.sparse(numFeatures, tfidfs.items())] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting the texts from previously created dataframe\n",
    "\n",
    "documents2 = by_words.select('text').rdd.map(list)\n",
    "\n",
    "def unpack(document):\n",
    "    texts = []\n",
    "    for term in document[0]:\n",
    "        #text = term\n",
    "        texts.append(term)\n",
    "    return texts\n",
    "\n",
    "documents3 = documents2.map(unpack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply the function to get TF_IDF sparse vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|        TFIDF_vector|\n",
      "+---+--------------------+\n",
      "|  0|(7435,[285,1364,1...|\n",
      "|  2|(7435,[140,285,46...|\n",
      "|  2|(7435,[140,285,46...|\n",
      "|  3|(7435,[75,285,506...|\n",
      "|  4|(7435,[632,1204,1...|\n",
      "|  5|(7435,[285,492,71...|\n",
      "|  6|(7435,[140,1448,3...|\n",
      "|  7|(7435,[285,889,12...|\n",
      "|  8|(7435,[1196,1364,...|\n",
      "| 10|(7435,[5972,6856]...|\n",
      "| 11|(7435,[2694,3102,...|\n",
      "| 11|(7435,[2694,3102,...|\n",
      "| 13|(7435,[1482,2760,...|\n",
      "| 15|(7435,[1364,1442,...|\n",
      "| 16|(7435,[1412,2389,...|\n",
      "| 18|(7435,[3440,4316]...|\n",
      "| 20|(7435,[2501,3260,...|\n",
      "| 22|(7435,[54,1267,13...|\n",
      "| 23|(7435,[1909,2009,...|\n",
      "| 25|(7435,[285,1364,3...|\n",
      "+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = ['text', 'TFIDF_vector']\n",
    "frequencyVectors = documents3.map(transform)\n",
    "final = frequencyVectors.toDF(columns).join(by_words, on='text', how='left').orderBy('id').select('id', 'TFIDF_vector')\n",
    "\n",
    "final.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sparse TF_IDF vector for the first row looks as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(7435, {285: 0.0402, 1364: 0.2468, 1482: 0.3047, 2760: 0.3047, 2941: 0.1043, 3396: 0.2125, 3737: 0.1713, 6357: 0.3047, 6747: 0.0284, 6856: 0.2151, 7280: 0.3047})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequencyVectors.take(1)[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "\n",
    "Reading text file into RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def clear_line(line):\n",
    "#    punct_removed = re.sub(r'[^\\w\\s]','',line)\n",
    "#    return punct_removed.lower().split(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read texts and add index to each document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['the',\n",
       "   'project',\n",
       "   'gutenberg',\n",
       "   'ebook',\n",
       "   'of',\n",
       "   'frankenstein',\n",
       "   'by',\n",
       "   'mary',\n",
       "   'wollstonecraft',\n",
       "   'godwin',\n",
       "   'shelley'],\n",
       "  0),\n",
       " ([], 1),\n",
       " (['this',\n",
       "   'ebook',\n",
       "   'is',\n",
       "   'for',\n",
       "   'the',\n",
       "   'use',\n",
       "   'of',\n",
       "   'anyone',\n",
       "   'anywhere',\n",
       "   'in',\n",
       "   'the',\n",
       "   'united',\n",
       "   'states',\n",
       "   'and'],\n",
       "  2)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddText = sc.textFile(f\"{filepath}\").map(lambda line: process_string(line))#.repartition(1).zipWithIndex().repartition(5)\n",
    "\n",
    "rdd_with_indicies = rddText.zipWithIndex()\n",
    "rdd_with_indicies.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And separatly let's make rdd without empty elements (and with no indexing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the',\n",
       "  'project',\n",
       "  'gutenberg',\n",
       "  'ebook',\n",
       "  'of',\n",
       "  'frankenstein',\n",
       "  'by',\n",
       "  'mary',\n",
       "  'wollstonecraft',\n",
       "  'godwin',\n",
       "  'shelley'],\n",
       " ['this',\n",
       "  'ebook',\n",
       "  'is',\n",
       "  'for',\n",
       "  'the',\n",
       "  'use',\n",
       "  'of',\n",
       "  'anyone',\n",
       "  'anywhere',\n",
       "  'in',\n",
       "  'the',\n",
       "  'united',\n",
       "  'states',\n",
       "  'and'],\n",
       " ['most',\n",
       "  'other',\n",
       "  'parts',\n",
       "  'of',\n",
       "  'the',\n",
       "  'world',\n",
       "  'at',\n",
       "  'no',\n",
       "  'cost',\n",
       "  'and',\n",
       "  'with',\n",
       "  'almost',\n",
       "  'no',\n",
       "  'restrictions']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_cleared = rddText.filter(bool)\n",
    "rdd_cleared.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate IDFs, let's save the total number of texts into `text_num`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6737"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_num = rdd_cleared.count()\n",
    "texts_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to count the number of texts, containing each word. Let's initialize counting by setting 1 to each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def counter(document):\n",
    "    lst = []\n",
    "    for word in document:\n",
    "        lst.append((word, 1))\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('the', 1),\n",
       "  ('project', 1),\n",
       "  ('gutenberg', 1),\n",
       "  ('ebook', 1),\n",
       "  ('of', 1),\n",
       "  ('frankenstein', 1),\n",
       "  ('by', 1),\n",
       "  ('mary', 1),\n",
       "  ('wollstonecraft', 1),\n",
       "  ('godwin', 1),\n",
       "  ('shelley', 1)],\n",
       " [('this', 1),\n",
       "  ('ebook', 1),\n",
       "  ('is', 1),\n",
       "  ('for', 1),\n",
       "  ('the', 1),\n",
       "  ('use', 1),\n",
       "  ('of', 1),\n",
       "  ('anyone', 1),\n",
       "  ('anywhere', 1),\n",
       "  ('in', 1),\n",
       "  ('the', 1),\n",
       "  ('united', 1),\n",
       "  ('states', 1),\n",
       "  ('and', 1)],\n",
       " [('most', 1),\n",
       "  ('other', 1),\n",
       "  ('parts', 1),\n",
       "  ('of', 1),\n",
       "  ('the', 1),\n",
       "  ('world', 1),\n",
       "  ('at', 1),\n",
       "  ('no', 1),\n",
       "  ('cost', 1),\n",
       "  ('and', 1),\n",
       "  ('with', 1),\n",
       "  ('almost', 1),\n",
       "  ('no', 1),\n",
       "  ('restrictions', 1)],\n",
       " [('whatsoever', 1),\n",
       "  ('you', 1),\n",
       "  ('may', 1),\n",
       "  ('copy', 1),\n",
       "  ('it', 1),\n",
       "  ('give', 1),\n",
       "  ('it', 1),\n",
       "  ('away', 1),\n",
       "  ('or', 1),\n",
       "  ('reuse', 1),\n",
       "  ('it', 1),\n",
       "  ('under', 1),\n",
       "  ('the', 1),\n",
       "  ('terms', 1)],\n",
       " [('of', 1),\n",
       "  ('the', 1),\n",
       "  ('project', 1),\n",
       "  ('gutenberg', 1),\n",
       "  ('license', 1),\n",
       "  ('included', 1),\n",
       "  ('with', 1),\n",
       "  ('this', 1),\n",
       "  ('ebook', 1),\n",
       "  ('or', 1),\n",
       "  ('online', 1),\n",
       "  ('at', 1)],\n",
       " [('wwwgutenbergorg', 1),\n",
       "  ('if', 1),\n",
       "  ('you', 1),\n",
       "  ('are', 1),\n",
       "  ('not', 1),\n",
       "  ('located', 1),\n",
       "  ('in', 1),\n",
       "  ('the', 1),\n",
       "  ('united', 1),\n",
       "  ('states', 1),\n",
       "  ('you', 1)],\n",
       " [('will', 1),\n",
       "  ('have', 1),\n",
       "  ('to', 1),\n",
       "  ('check', 1),\n",
       "  ('the', 1),\n",
       "  ('laws', 1),\n",
       "  ('of', 1),\n",
       "  ('the', 1),\n",
       "  ('country', 1),\n",
       "  ('where', 1),\n",
       "  ('you', 1),\n",
       "  ('are', 1),\n",
       "  ('located', 1),\n",
       "  ('before', 1)],\n",
       " [('using', 1), ('this', 1), ('ebook', 1)],\n",
       " [('title', 1), ('frankenstein', 1)],\n",
       " [('or', 1), ('the', 1), ('modern', 1), ('prometheus', 1)],\n",
       " [('author', 1),\n",
       "  ('mary', 1),\n",
       "  ('wollstonecraft', 1),\n",
       "  ('godwin', 1),\n",
       "  ('shelley', 1)],\n",
       " [('release', 1), ('date', 1), ('ebook', 1)],\n",
       " [('most', 1), ('recently', 1), ('updated', 1), ('november', 1)],\n",
       " [('language', 1), ('english', 1)],\n",
       " [('character', 1), ('set', 1), ('encoding', 1), ('utf', 1)],\n",
       " [('produced', 1),\n",
       "  ('by', 1),\n",
       "  ('judith', 1),\n",
       "  ('boss', 1),\n",
       "  ('christy', 1),\n",
       "  ('phillips', 1),\n",
       "  ('lynn', 1),\n",
       "  ('hanninen', 1),\n",
       "  ('and', 1),\n",
       "  ('david', 1),\n",
       "  ('meltzer', 1),\n",
       "  ('html', 1),\n",
       "  ('version', 1),\n",
       "  ('by', 1),\n",
       "  ('al', 1),\n",
       "  ('haines', 1)],\n",
       " [('further', 1),\n",
       "  ('corrections', 1),\n",
       "  ('by', 1),\n",
       "  ('menno', 1),\n",
       "  ('de', 1),\n",
       "  ('leeuw', 1)],\n",
       " [('start', 1),\n",
       "  ('of', 1),\n",
       "  ('the', 1),\n",
       "  ('project', 1),\n",
       "  ('gutenberg', 1),\n",
       "  ('ebook', 1),\n",
       "  ('frankenstein', 1)],\n",
       " [('frankenstein', 1)],\n",
       " [('or', 1), ('the', 1), ('modern', 1), ('prometheus', 1)]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_initializer = rdd_cleared.map(lambda word: counter(word))\n",
    "count_initializer.take(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we remove duplicates in each text and then sum up values with the same keys, we will get exactly the number of documents, containing each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_duplicates(document):\n",
    "    return list(dict.fromkeys(document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('the', 1),\n",
       "  ('project', 1),\n",
       "  ('gutenberg', 1),\n",
       "  ('ebook', 1),\n",
       "  ('of', 1),\n",
       "  ('frankenstein', 1),\n",
       "  ('by', 1),\n",
       "  ('mary', 1),\n",
       "  ('wollstonecraft', 1),\n",
       "  ('godwin', 1),\n",
       "  ('shelley', 1)],\n",
       " [('this', 1),\n",
       "  ('ebook', 1),\n",
       "  ('is', 1),\n",
       "  ('for', 1),\n",
       "  ('the', 1),\n",
       "  ('use', 1),\n",
       "  ('of', 1),\n",
       "  ('anyone', 1),\n",
       "  ('anywhere', 1),\n",
       "  ('in', 1),\n",
       "  ('united', 1),\n",
       "  ('states', 1),\n",
       "  ('and', 1)],\n",
       " [('most', 1),\n",
       "  ('other', 1),\n",
       "  ('parts', 1),\n",
       "  ('of', 1),\n",
       "  ('the', 1),\n",
       "  ('world', 1),\n",
       "  ('at', 1),\n",
       "  ('no', 1),\n",
       "  ('cost', 1),\n",
       "  ('and', 1),\n",
       "  ('with', 1),\n",
       "  ('almost', 1),\n",
       "  ('restrictions', 1)],\n",
       " [('whatsoever', 1),\n",
       "  ('you', 1),\n",
       "  ('may', 1),\n",
       "  ('copy', 1),\n",
       "  ('it', 1),\n",
       "  ('give', 1),\n",
       "  ('away', 1),\n",
       "  ('or', 1),\n",
       "  ('reuse', 1),\n",
       "  ('under', 1),\n",
       "  ('the', 1),\n",
       "  ('terms', 1)],\n",
       " [('of', 1),\n",
       "  ('the', 1),\n",
       "  ('project', 1),\n",
       "  ('gutenberg', 1),\n",
       "  ('license', 1),\n",
       "  ('included', 1),\n",
       "  ('with', 1),\n",
       "  ('this', 1),\n",
       "  ('ebook', 1),\n",
       "  ('or', 1),\n",
       "  ('online', 1),\n",
       "  ('at', 1)],\n",
       " [('wwwgutenbergorg', 1),\n",
       "  ('if', 1),\n",
       "  ('you', 1),\n",
       "  ('are', 1),\n",
       "  ('not', 1),\n",
       "  ('located', 1),\n",
       "  ('in', 1),\n",
       "  ('the', 1),\n",
       "  ('united', 1),\n",
       "  ('states', 1)],\n",
       " [('will', 1),\n",
       "  ('have', 1),\n",
       "  ('to', 1),\n",
       "  ('check', 1),\n",
       "  ('the', 1),\n",
       "  ('laws', 1),\n",
       "  ('of', 1),\n",
       "  ('country', 1),\n",
       "  ('where', 1),\n",
       "  ('you', 1),\n",
       "  ('are', 1),\n",
       "  ('located', 1),\n",
       "  ('before', 1)],\n",
       " [('using', 1), ('this', 1), ('ebook', 1)],\n",
       " [('title', 1), ('frankenstein', 1)],\n",
       " [('or', 1), ('the', 1), ('modern', 1), ('prometheus', 1)],\n",
       " [('author', 1),\n",
       "  ('mary', 1),\n",
       "  ('wollstonecraft', 1),\n",
       "  ('godwin', 1),\n",
       "  ('shelley', 1)],\n",
       " [('release', 1), ('date', 1), ('ebook', 1)],\n",
       " [('most', 1), ('recently', 1), ('updated', 1), ('november', 1)],\n",
       " [('language', 1), ('english', 1)],\n",
       " [('character', 1), ('set', 1), ('encoding', 1), ('utf', 1)],\n",
       " [('produced', 1),\n",
       "  ('by', 1),\n",
       "  ('judith', 1),\n",
       "  ('boss', 1),\n",
       "  ('christy', 1),\n",
       "  ('phillips', 1),\n",
       "  ('lynn', 1),\n",
       "  ('hanninen', 1),\n",
       "  ('and', 1),\n",
       "  ('david', 1),\n",
       "  ('meltzer', 1),\n",
       "  ('html', 1),\n",
       "  ('version', 1),\n",
       "  ('al', 1),\n",
       "  ('haines', 1)],\n",
       " [('further', 1),\n",
       "  ('corrections', 1),\n",
       "  ('by', 1),\n",
       "  ('menno', 1),\n",
       "  ('de', 1),\n",
       "  ('leeuw', 1)],\n",
       " [('start', 1),\n",
       "  ('of', 1),\n",
       "  ('the', 1),\n",
       "  ('project', 1),\n",
       "  ('gutenberg', 1),\n",
       "  ('ebook', 1),\n",
       "  ('frankenstein', 1)],\n",
       " [('frankenstein', 1)],\n",
       " [('or', 1), ('the', 1), ('modern', 1), ('prometheus', 1)]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter_dropped_duplicates = count_initializer.map(lambda word: delete_duplicates(word))\n",
    "counter_dropped_duplicates.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('project', 88),\n",
       " ('gutenberg', 31),\n",
       " ('ebook', 13),\n",
       " ('of', 2435),\n",
       " ('mary', 3),\n",
       " ('shelley', 3),\n",
       " ('this', 442),\n",
       " ('is', 305),\n",
       " ('use', 21),\n",
       " ('anyone', 9),\n",
       " ('anywhere', 2),\n",
       " ('in', 1127),\n",
       " ('united', 20),\n",
       " ('other', 91),\n",
       " ('world', 47),\n",
       " ('at', 318),\n",
       " ('no', 171),\n",
       " ('restrictions', 2),\n",
       " ('whatsoever', 3),\n",
       " ('may', 108)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_with_word = counter_dropped_duplicates.flatMap(lambda value: value).reduceByKey(lambda a, b: a + b)\n",
    "texts_with_word.take(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we got the number of documents `df`, containing each word. Comparing the results with the previously obtained in Part 1, we make sure that the calculations are correct. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to calculate IDF, we need to devide the total number of texts `D` by the number of documents with each word `df` and take 10-based logarithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8412473174446735\n",
      "3.226406556024716\n"
     ]
    }
   ],
   "source": [
    "idfs_rdd = texts_with_word.map(lambda x: {x[0] : np.log10(texts_num/x[1])})\n",
    "pre_dict = idfs_rdd.collect()\n",
    "\n",
    "dict_rdd = {}\n",
    "for element in pre_dict:\n",
    "    dict_rdd.update(element)\n",
    "    \n",
    "print(dict_rdd['that'])\n",
    "print(dict_rdd['online'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's apply the same function as was used in the Part 1 (only with a different dictionary of IDF) to get the sparse TF-IDF vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform2(document):\n",
    "    freq = {}\n",
    "    tfidfs = {}\n",
    "    for term in document:\n",
    "        i = hash(term) % numFeatures\n",
    "        freq[i] = freq.get(i, 0) + 1.0\n",
    "        tfidfs[i] = freq[i]/len(document) * dict_rdd[term]\n",
    "    return [document, Vectors.sparse(numFeatures, tfidfs.items())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first RDD row:\n",
      "[[['the', 'project', 'gutenberg', 'ebook', 'of', 'frankenstein', 'by', 'mary', 'wollstonecraft', 'godwin', 'shelley'], SparseVector(7435, {285: 0.0402, 1364: 0.2468, 1482: 0.3047, 2760: 0.3047, 2941: 0.1043, 3396: 0.2125, 3737: 0.1713, 6357: 0.3047, 6747: 0.0284, 6856: 0.2151, 7280: 0.3047})]]\n",
      "\n",
      "Dataframe\n",
      "+--------------------+--------------------+\n",
      "|                text|        TFIDF_vector|\n",
      "+--------------------+--------------------+\n",
      "|[the, project, gu...|(7435,[285,1364,1...|\n",
      "|[this, ebook, is,...|(7435,[140,285,46...|\n",
      "|[most, other, par...|(7435,[75,285,506...|\n",
      "|[whatsoever, you,...|(7435,[632,1204,1...|\n",
      "|[of, the, project...|(7435,[285,492,71...|\n",
      "|[wwwgutenbergorg,...|(7435,[140,1448,3...|\n",
      "|[will, have, to, ...|(7435,[285,889,12...|\n",
      "|[using, this, ebook]|(7435,[1196,1364,...|\n",
      "|[title, frankenst...|(7435,[5972,6856]...|\n",
      "|[or, the, modern,...|(7435,[2694,3102,...|\n",
      "|[author, mary, wo...|(7435,[1482,2760,...|\n",
      "|[release, date, e...|(7435,[1364,1442,...|\n",
      "|[most, recently, ...|(7435,[1412,2389,...|\n",
      "| [language, english]|(7435,[3440,4316]...|\n",
      "|[character, set, ...|(7435,[2501,3260,...|\n",
      "|[produced, by, ju...|(7435,[54,1267,13...|\n",
      "|[further, correct...|(7435,[1909,2009,...|\n",
      "|[start, of, the, ...|(7435,[285,1364,3...|\n",
      "|      [frankenstein]|(7435,[6856],[2.3...|\n",
      "|[or, the, modern,...|(7435,[2694,3102,...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "frequencyVectors2 = rdd_cleared.map(transform2)\n",
    "final2 = frequencyVectors2.toDF(columns)\n",
    "\n",
    "print('The first RDD row:')\n",
    "print(frequencyVectors2.take(1))\n",
    "print('')\n",
    "print('Dataframe')\n",
    "print(final2.show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, let's bring the text indices back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|        TFIDF_vector|\n",
      "+---+--------------------+\n",
      "|  0|(7435,[285,1364,1...|\n",
      "|  2|(7435,[140,285,46...|\n",
      "|  2|(7435,[140,285,46...|\n",
      "|  3|(7435,[75,285,506...|\n",
      "|  4|(7435,[632,1204,1...|\n",
      "|  5|(7435,[285,492,71...|\n",
      "|  6|(7435,[140,1448,3...|\n",
      "|  7|(7435,[285,889,12...|\n",
      "|  8|(7435,[1196,1364,...|\n",
      "| 10|(7435,[5972,6856]...|\n",
      "| 11|(7435,[2694,3102,...|\n",
      "| 11|(7435,[2694,3102,...|\n",
      "| 13|(7435,[1482,2760,...|\n",
      "| 15|(7435,[1364,1442,...|\n",
      "| 16|(7435,[1412,2389,...|\n",
      "| 18|(7435,[3440,4316]...|\n",
      "| 20|(7435,[2501,3260,...|\n",
      "| 22|(7435,[54,1267,13...|\n",
      "| 23|(7435,[1909,2009,...|\n",
      "| 25|(7435,[285,1364,3...|\n",
      "+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final2.join(rdd_with_indicies.toDF(['text', 'id']), on='text', how='left').orderBy('id').select('id', 'TFIDF_vector').show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
